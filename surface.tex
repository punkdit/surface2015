%\documentclass[12pt,notitlepage,aps,pra,longbibliography,nofootinbib,tightenlines]{revtex4}
%\documentclass[12pt,notitlepage,longbibliography,nofootinbib,tightenlines]{revtex4-1}
%\documentclass[12pt,notitlepage,longbibliography,nofootinbib,tightenlines]{revtex4-1}

%\documentclass[12pt,a4]{revtex4}
\documentclass[11pt]{article}
%\documentclass[11pt, twocolumn]{article}


% xelatex:
\usepackage{fontspec}
\defaultfontfeatures{Ligatures=TeX}
%\usepackage[small,sf,bf]{titlesec}

%\setromanfont{DejaVu Serif}
%\setromanfont{Droid Serif}

%\setromanfont{Gentium} % nice! a bit fluffy
\setromanfont{Gentium Book Basic} % more bold

%\setromanfont{Noto Serif} % a bit thick
%\setromanfont{Utopia}


%\usepackage{epsf}
\usepackage{amsmath}
\usepackage{color}
\usepackage{natbib}
%\usepackage{cite}

\RequirePackage{amsmath}
\RequirePackage{amssymb}
\RequirePackage{amsthm}
%\RequirePackage{algorithmic}
%\RequirePackage{algorithm}
%\RequirePackage{theorem}
%\RequirePackage{eucal}
\RequirePackage{color}
\RequirePackage{url}
\RequirePackage{mdwlist}

\RequirePackage[all]{xy}
\CompileMatrices
\RequirePackage{hyperref}
\RequirePackage{graphicx}
%\RequirePackage[dvips]{geometry}


\makeatletter
\newcommand{\pushright}[1]{\ifmeasuring@#1\else\omit\hfill$\displaystyle#1$\fi\ignorespaces}
\newcommand{\pushleft}[1]{\ifmeasuring@#1\else\omit$\displaystyle#1$\hfill\fi\ignorespaces}
\makeatother


\begin{document}

%\title{The surface betrays what lies beneath}
%\title{Show me the morphism}
%\title{When the surface betrays what lies beneath}
%\title{Path-integrals are everywhere}
%\title{The meaning is in the arrows between objects, not in the objects themselves}
%\title{Design patterns in message passing}
\title{The Physics of Meaning}

\author{Simon Burton}
%\affiliation{Centre for Engineered Quantum Systems, School of Physics, The University of Sydney}

\date{\today}

%\begin{abstract}
%We make a compendium of many uses of a simple idea.
%Perhaps it is the only really great calculation.
%\end{abstract}

\maketitle


\def\N{\mathbb N}
\def\Z{\mathbb Z}
\def\R{\mathbb R}
\def\C{\mathbb C}
\def\Expect{\mathbb E}
\def\Ind{\mathbb I}
\def\Complex{\mathbb{C}}
\def\GL{\mathrm{GL}}
\def\half{\frac{1}{2}}
\def\todo#1{\emph{(XXX #1 XXX)}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

%\section{Introduction}

The so-called ``Generalized Distributive Law''
\cite{Aji2000}
explores the ramifications of the innocent looking
formula $ab+ac = a(b+c).$
These include such gems as the fast Fourier transform
and belief propagation.
We can also think of multiplication by $a$ as a function $f$
and rewrite this equation as 
$$
    f(b)\hat{+}f(c)=f(b+c) \ \ \ \ \ \ \ \ \ \ \ \mbox{(M)}
$$
This is the equation for a homomorphism, or representation.
It says we can represent addition (on the right-hand side)
as some other kind of addition, $\hat{+}.$
%The simple distributive law states that we can zoom in or out

A technique that shows up in various places:
mathematics, physics, computer science, statistics, economics...

% univalent mathematics identifies isomorphic objects,
% these path-integrals also have a kind of contractive ``throat''
% where many possibilities are identified.. is this the same business?


Coming more from a programming background, I've also had a keen
interest in networks, and how ``information'' propagates through networks.

In particular, I've long been fascinated by a seeming analogy
between path integrals (in physics) and certain optimization problems
in computer science, eg. shortest path in a network.

This analogy seems to gobble up many other algorithms, depending on how
loose one is being. From past experience working in a machine
learning context, things like Belief Propagation and also
the Fast Fourier Transform fit in. According to those guys,
the key is a ``generalized distributive law'', which I think
amounts to what mathematicians would call a (semi-)ring, or rig.

But it seems that this analogy can be pushed much further.
For example, combinatorial species seem to fit. Here the ``network''
has as vertices the set of all possible structures that a species
can produce (i'm being a bit loose here) and the edges of the
network come from differentiating: the differential of
a species ``constructs'' larger structures from smaller ones.
This looks like a ``wavefront'', the size of which is the
number of structures on an n-element set.

This fits in nicely with the idea of ``zippers'' in 
computer science. In pure functional programming all
data structures are immutable, which makes programs easier
to reason about (and so less bugs!) but now the programmer
has to work harder, for example how does one insert an
element into an immutable list of elements? The idea behind
zippers is essentially the same as for combinatorial
species. They use the differential to keep track of a
``hole'' or ``context'' in a given data structure. This 
(along with some other magic i don't quite understand)
now looks like another kind of ``wavefront'' propagating 
over a network of immutable values as the computation proceeds.

Automatic Differentiation is another area of importance
for machine learning. Physicists would
call this perturbation theory. It comes in two
flavours: forward and reverse. The forward version
is the one I understand. Here the network is the
``parse tree'' of some concrete expression, eg. $(x^2)*sin(x)$.
Instead of differentiating this directly (symbolically),
we substitute $x$ with $x+\varepsilon$, and ignore higher powers of $\varepsilon$.
It feels like this should fit in with the above analogy,
since we replace a kind of complicated path integral 
(symbolic differentiation), 
with a (much easier to deal with) propagating wavefront.

One other oddball involves logic and cut-elimination.
It seems like this might just be an example of using
the ``and, or'' ring of ``true, false''. In brief, Gentzen
style sequent calculus is propagating a wavefront across
a natural deduction proof network. Cut-elimination is
the key that makes this thing work.

Many similar ideas show up in different guises.
*) For instance, green's functions appear in many of these applications,
and look like some kind of ``dual'' to the vertices in the network.
*) Things tend to break down in the presence of loops: eg. 
belief propagation works perfectly on trees, but not so well on
networks with (small) loops. Quantum field theory would be
great except we can't evaluate Feynman diagrams that have
loops.
*) Two modes: forward and backward. In functional programming
this shows up as ``eager'' versus ``lazy'' evaluation.

Pondering all of this it seems that these wavefronts could
all be some kind of arrows in a category. Indeed, distributivity
is the condition for a homomorphism (multiplication by a scalar
is a homomorphism of the additive group.) So perhaps there
is some kind of generalized ``arrow'' that can serve.

%I keep reading your 
%blog posts about network theory and it finally has occurred to me that
%maybe spans can do this. Boy was I surprised to find that you
%and the other n-category guys have already worked alot of this
%out, and I was astounded to find the Russian school of ``idempotent
%analysis''. Thankyou for writing all of that down!

It seems to me that physicists should pay more attention to
all of this. For me it even provides a motivation for quantum theory,
at a time when physicists don't have any good intuition
for why the laws of quantum physics are the way they are.

Machine learning research appears to progress in a succession of
exciting breakthroughs followed by chilly winters of not much progress.
One of the first such breakthroughs was ``dynamic programming'', which
seems to have been enshrined into the field of ``operations research''.

The matrix power method (for finding the eigenvector with the largest eigenvalue)
is also a kind of path-integral. Should that be added to the list? This is
the essence of the google page-rank algorithm. I guess this is a kind of
``transfer matrix''.

The so-called ``renormalization group'' in statistical physics also seems to
crucially apply the distributive law, at least in 1 dimensional lattice systems. I'm not
so sure about higher dimensional lattices, where there are alot of little loops
that once again tends to limit the effectiveness of these schemes. 

Physicists use a kind of belief propagation, they call it the ``cavity method''. 
It works well in one dimensional lattices. The ``Bethe Ansatz'' works perfectly.
(Quantum Inverse Scattering)
Replica theory, and ``replica symmetry breaking'' has no good mathematical basis.
Ultrametric spaces show up here. These are metric spaces where
the plus in the triangle inequality is replaced with a max (as in the max-plus semi-ring).
Apparently, Parisi was originally motivated by considering
the permutation group $S_n$ when $n<1$, particularly the limit as $n$ goes to zero.

Recursive domain equations... least fixed points in POSETs (DCPOs..)



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Fast Fourier transform}

% Apparently invented by Gauss:
% http://www.cis.rit.edu/class/simg716/Gauss_History_FFT.pdf


%The discrete Fourier transform of a sequence $\{x_j\}_0^{N-1}$
%is given by
%$$
%%    y_k = \sum_{j=0}^{N-1} x_j e^{2\pi i k \frac{j}{N}}, \ \ k=0,...,N-1.
%    y_k = \sum_{j=0}^{N-1} x_j \omega^{kj}, \ \ k=0,...,N-1.
%$$
%where $\omega=e^{2\pi i/N}.$
%If $N$ is a composite number $N=N_1 N_2$ then we can re-express this sum as
%\begin{align*}
%    y_k &= \sum_{j_1=0}^{N_1-1} \sum_{j_2=0}^{N_2-1} x_{j_1 N_2 + j_2}
%            \omega^{k (j_1 N_2 + j_2)}, \ \ k=0,...,N-1. \\
%        &= \sum_{j_1=0}^{N_1-1}
%            \omega^{k {j_1 N_2}}
%            \sum_{j_2=0}^{N_2-1} x_{j_1 N_2 + j_2}
%            \omega^{k {j_2}}, \ \ k=0,...,N-1.
%\end{align*}
The discrete Fourier transform of a function $f:\Z_n\to \R$
is another function $\hat{f}:\Z_n\to \R$ given by
$$
    \hat{f}(k) := \sum_{j=0}^{n-1} \omega^{jk}f(j),%\ \ \ \mathrm{with}\ \ \omega:=e^{\frac{2\ph i}{n}}.
$$
where $\omega=e^{2\pi i/n}.$
If $n$ is a composite number $n=pq$ then we
write $j=j_1 q + j_2$ with $j_1 = 0,...,p-1$ and $j_2 = 0,...,q-1$
and also $f(j_1, j_2):=f(j_1 q + j_2)$
then
\begin{align*}
        \hat{f}(k) = \sum_{j_2=0}^{q-1} \omega^{j_2k} \sum_{j_1=0}^{p-1} \omega^{j_1kq} f(j_1,j_2) 
\end{align*}
If we rewrite $k$ using
$k=k_1 q + k_2$ with $k_1 = 0,...,p-1$ and $k_2 = 0,...,q-1$
and note that 
$\omega^{j_1kq}=\omega^{j_1(k_2p+k_1)q}=\omega^{j_1k_1q}:$
$$
        \hat{f}(k_1, k_2) = \sum_{j_2=0}^{q-1} \omega^{j_2(k_2 p+k_1)} \sum_{j_1=0}^{p-1} \omega^{j_1k_1q} f(j_1,j_2)
$$
The point here is that the inner sum does not depend on $k_2:$
\begin{align*}
        \hat{f}(k_1, k_2) &= \sum_{j_2=0}^{q-1} \omega^{j_2(k_2 p+k_1)} g(k_1, j_2) \\
        &\ \ \mathrm{with} \ \ g(k_1,j_2) := \sum_{j_1=0}^{p-1} \omega^{j_1k_1q} f(j_1,j_2)
\end{align*}
so we can re-use the calculation of $g(k_1, j_2)$ 
in each of the terms $\hat{f}(k_1, k_2)$ for $k_2=0,...,q-1.$


%Displaying this graphically. Bratelli diagram, butterfly network

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Cavity method}

% Min-sum equation

We consider an Ising model with pairwise energy terms given by

$$
    E_{ij} = -\sigma_i\sigma_j
$$

The min-sum algorithm is a type of message passing algorithm.
Each spin sends messages to its neighbours.
These messages are quotations of the form 
``If you do x then I will charge you y''.
The message from spin $i$ to spin $j$ at time $t$ is denoted
$e_{ij}^{(t)}(\sigma_j)$.
This message tells spin $j$ what the cost if each of
its actions will be (according to spin $i$).
Spin $i$ will consult each of its other neighbours 
(apart from $\sigma_j$) and then minimize its own cost
(in response to $\sigma_j$):

$$
e_{ij}^{(t+1)}(\sigma_j) = \min_{\sigma_i}\bigl[ E_{ij}(\sigma_i, \sigma_j)
  + \sum_{k\in \partial i/j} e_{ki}^{(t)}(\sigma_i) \bigr]
$$

The final ``marginal'' cost for each spin is given by

$$
    E_j^{(t+1)}(\sigma_j) = \sum_{i\in \partial j} e_{ij}^{(t)}(\sigma_j)
$$

which can be used to find energy minima:

$$
    \sigma_j = \text{argmin} E_j(\sigma_j).
$$

It's possible to add arbitrary constants to each
message:

$$
e_{ij}^{(t+1)}(\sigma_j) = \min_{\sigma_i}\bigl[ E_{ij}(\sigma_i, \sigma_j)
  + \sum_{k\in \partial i/j} e_{ki}^{(t)}(\sigma_i) \bigr] + C_{ij},
$$

so we can ``normalize'' each message passed so
that the minimum value is zero: $\min e_{ij}^{(t)}(\sigma_j) = 0.$

The corresponding fixed point equations are known
as the {\bf energetic cavity equations} in statistical physics.
(The ``cavity'' refers to how the equations successively
remove one variable.)

This algorithm is exact on models with a tree topology,
but can get stuck on models with loops, especially small
loops (eg. two dimensional lattices). This is clear
because the algorithm essentially models zero (?) temperature
dynamics, and we know that the ising model has no
stable configurations for linear or treelike topologies.


Bethe Anatz (solves Heisenberg in 1D). 
Is there a forward \& reverse?

\section{Suzuki-Trotter formula}

This is a way of writing the exponential of a sum of operators
as a path integral:
$$
\exp(A+B) = \lim_{n\to\infty} \Bigl(\exp(A/n)\exp(B/n)\Bigr)^n.
$$
 

\section{Belief propagation}


% Sum-product equation

We consider now the thermal distribution:

$$
    p(\sigma) = \prod_{ij} e^{-E_{ij}(\sigma_i, \sigma_j)}
$$

The factors are:

$$
\phi_{ij}(\sigma_i, \sigma_j) = e^{-E_{ij}(\sigma_i, \sigma_j)}
$$

Now the messages we pass will be ``beliefs'', $b_{ij}$.

$$
b_{ij}^{(t+1)}(\sigma_j) = \sum_{\sigma_i}\bigl[ \phi_{ij}(\sigma_i, \sigma_j)
  + \prod_{k\in \partial i/j} b_{ki}^{(t)}(\sigma_i) \bigr]
$$

The final marginal for each spin is given by

$$
    b_j^{(t+1)}(\sigma_j) = \prod_{i\in \partial j} b_{ij}^{(t)}(\sigma_j)
$$

These equations are structurally the same as the
min-sum equations, with sum replaced by product and
min replaced by sum.




Economics is a min-sum message passing algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Dijkstra's algorithm}

Let $G$ be a finite graph: $G = (V, E).$
%Choose a POSET $\P$ with unique 
For $x,y\in V$ Write $x\sim y$ when there
is an edge between $x$ and $y.$ % or when $x=y.$
%Let $\P=\mathbb Z \cup \{\infty\}$ be the ordered set of integers with a top element.
Define the set of \emph{states} as %``height'' functions
the finite dimensional vector space
$A = \{ f | f : V \to \R \}.$

The \emph{message passing} function
$T:A\to A$ is defined by
$$(Tf)(x) = \min\{f(x), \min_{y\sim x} f(y)+1\}.$$

{\bf Lemma.} There exists $n$ such that $T^{n+1} = T^n.$

{\bf Proof.}
%$A$ is a partially ordered set, using
%the pointwise order:
%$f\le g$ iff $f(x)\le g(x)\ \forall x\in V.$
%$T$ is monotonic non-increasing on $A.$
%For any $f\in A$ we have $g\le f$
%where $g$ is a constant function defined by $g(x)=\min_y f(y).$
%Therefore the set $\{f, Tf, T^2f,...\}$ is finite.
%\todo{Insert elegant proof here. Need to use $G$ finite.}
%The proof is by induction.
This is Dijkstra's algorithm \cite{Dijkstra1959}.
Without loss of generality we may take
$G$ to be connected.
Choose $f\in A.$
Define $f_i = T^i f.$
%We will show that $T^{n+1}f = T^n f$
We will show that $f_{n+1} = f_n$
where $n$ depends only on $G.$
%we will partition the vertices $V$ into three sets:
For each iteration, $i=1,...,n$
we will maintain a subset $S_i\subset V$
of ``accepted'' vertices:
if $x\in S_i$ then $f_i(y)-f_i(x)\le 1$ for all $y\sim x.$
Begin with $S_1=\arg\min(f).$
Then at each step we expand $S_i$ using its
neighbourhood: $S_{i+1}=S_i\cup \partial S_i.$
If we choose $n=|V|$ then $S_n=V$ and we are done.
\qed

%Let $n$ be the minimum $n$ such that $T^{n+1}=T^n.$ 

\def\Fix{\mathrm{Fix}}

%Any $f\in  A$ such that $Tf=f$ is called \emph{stationary.}
%The set of such states we denote $\Fix(T).$
The set of fixed points of $T$ we denote as
$\Fix(T):=\{f\in A|Tf=f\}.$
%Any constant function  is stationary.
%The functions furthest away from being stationary
%are the $f$ that have $T^nf\ne T^{n-1}f.$
%We call these the \emph{points} of $T.$
%We would like to identify these functions with
%the vertices of the graph, but there are too many such $f$.
It can be seen that
a state $f$ is fixed by $T$ iff
$f(x)\le f(y)+1,\ \forall x\sim y.$
Direct computation shows that
%set of stationary states is convex:
$\Fix(T)$ is convex:
given $f$ and $g$ both fixed by $T$,
and $\alpha\in[0, 1]$ then $\alpha f + (1-\alpha) g$
is fixed by $T$.

We declare two states $f$ and $g$
to be equivelant whenever $f-g$ is a constant function.
%We define an equivalence relation on 
%states $f$ and $g$ by 
%declaring $f\sim g$ when $f-g$ is a constant state. 
This equivalence relation defines a projection $P,$
and $P(\Fix(T))$ is now a compact convex subset of
a finite dimensional real vector space.
\todo{looks like $A$ could be defined as affine ?}
Furthermore, this set is the intersection
of a finite set of closed half-spaces and
so has finitely many extreme points.
For each edge $x\sim y$ of the graph
we get a closed half space
$f(x)\le f(y)+1.$
This half space survives the action of $P$
because we can rewrite it as $f(x)-f(y)\le 1.$

For each extreme point $f$ of $P(\Fix(T))$ 
we will associate a directed graph $D_f$
as follows.
$D_f$ has the same vertex set as $G$.
$f$ will saturate a set of inequalities,
$f(x)-f(y)=1$
and this gives a directed edge $y\to x$ in $D_f.$
%This set must touch every vertex of $G.$
Every vertex participates in at least one
directed edge.
A \emph{path} in $D$ is a directed sequence of edges.
There are no loops (closed paths) in $D$.

{\bf Theorem.} Every $D$ path from $x$ to $y$ has
minimal length among all paths in $G$ from $x$ to $y.$

Conversely, to find a path of minimal length 
from $x$ to $y$ we set $f$ to be the ``indicator''
function for $x$ and then $T^nf$ will correspond
to an extremal point of $P(Fix(T))$ whose directed
graph contains all paths from $x$ to $y$ of minimal length in $G.$
The \emph{indicator} function for \ $x\in V$ \ is \ $\delta_x:V\to \R$ 
\ defined as\ $\delta_x(x)=0$\ and\ $\delta_x(y)=\mathrm{width}(G)$
\ for\ $y\ne x.$

Question: count the number of directed spanning trees in an undirected graph.
% Combinatorial species

\emph{Alternative formulation:}
a vector in a graph $G=(V,E)$ is a vertex, edge pair $(v,e)$ with $v\in e.$
The tangent space at a vertex $v$ is the set of all vectors $(v,e)$
at $v$.
Tanget vectors act on $A$...
Extremal points of $P(Fix(T))$ give discrete morse function...
what are the critical points?
Critical vertices are leaves, critical edges are those that cause loops... (check...)

% greedy algorithms & Matriods


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Dynamic programming}

techniques having to do with memoization 
in order to help an algorithm identify overlapping subproblems and avoid re-computing them.

Linear programming, simplex method (duality)

Approximate Dynamic programming (Reinforcement learning)
% https://webdocs.cs.ualberta.ca/~bowling/papers/07adprl-dualrl.pdf

Eikonal equation

Bellman's equation ---> Hamilton-Jacobi equations ---> Schrodinger (wave) Equation

% Recursive domain equations
% Combinatorial species (monads?)

Maximising buying and selling a price function $f:[0,1]\to \R$:
    The solution is to buy the local minima and sell the
    local maxima. Can get this via a test function and
    integration by parts. This is a morse function, with
    critical points being the trades.
    Is this also the dynamic programming solution?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Huygen's principle}

Path integrals, calculus of variations

Integration by parts involves a crucial use of the distributive law.

Stoke's theorem ? % Chern-Simons ? AdS-CFT correspondence ?

Green's functions,
boundary element method: http://george-and-bem.ce.umn.edu/fundamental.html

% Wave equation, Elliptic PDE's, Sobolev spaces...??? Elliptic PDE's represent
% static sollutions (eg. laplace/poisson equation)..

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Automatic differentiation}

Differentiation distributes
multiplicatively over function
composition:
$$
    D(f\circ g)(x) = Df(g(x))\cdot Dg(x)
$$

forward and reverse...

%\input{autodiff.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Linear recurrence relations}

The fibonacci series:
$$
    a_{n+2} = a_n + a_{n+1}
$$
Changing the notation somewhat, for any natural number $n$
we define the successor of $n$ as $Sn := n+1.$
We rewrite the recurrence relation in terms of a function $f:\N\to\R$
$$
    f(SSn) = f(Sn) + f(n).
$$
Pushing the analogy, and squinting a bit, this
could be called ``$f$ distributes linearly over $S.$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Convexity}
%http://arxiv.org/pdf/0903.5522v3.pdf

Addition distributes additively over convex
combinations:
$$
x + \frac{a+b}{2} = \frac{(x+a) + (x+b)}{2}.
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Markov processes}

Given a sequence of random variables $x_1,...,x_n$
the markov property is:
$$
    p(x_n|x_1,...,x_{n-1}) = p(x_n|x_{n-1}).
$$
Applied to a joint distribution we get:
\begin{align*}
    p(x_n,...,x_1) &= p(x_n|x_{n-1},...x_1) p(x_{n-1}|x_{n-2}...x_1)...p(x_1)\\
                    &= p(x_n|x_{n-1}) p(x_{n-1}|x_{n-2})...p(x_1).
\end{align*}
Loosely, $p$ distributes multiplicatively over joints.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Logic: cut-elimination}

Natural deduction, Gentzen's sequent calculus..

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Combinatorics...}

path counting, transfer matrix...
generating functions? 
creation/annihilation operators?
denotational semantics?

zippers and differentiating data structures

% Unitary evolution is ``immutabile" evolution of quantum state... 
% Can we use zippers to define a kind of measurement process?

% Google pagerank, power method, transfer matrix
% Word indexes: trie's, suffix trees, ...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Real-space renormalization group}

%works in 1d... reverse is zoom-in ?

The spins $\sigma$ are distributed according
to the Gibb's distribution:
\begin{align*}
p(\sigma) &= {{1} \over {Z}}e^{H(\sigma)}
\end{align*}

with normalizing constant:
\begin{align*}
Z &= \sum_{\sigma} e^{H(\sigma)}.
\end{align*}

We introduce more random variables $\mu$ and
use Baye's rule:
\begin{align*}
p(\sigma | \mu) &= {{1} \over {p(\mu)}}p(\mu | \sigma)p(\sigma)
\end{align*}

Now we split the hamiltonian into two terms
\begin{align*}
H &= H_{0}+V
\end{align*}

so that
\begin{align*}
p(\sigma | \mu) &= {{1} \over {p(\mu)}}p(\mu | \sigma){{1} \over {Z}}e^{H_{0}+V},
\end{align*}

and define the following conditional probability:
\begin{align*}
p_{0}(\sigma | \mu) &= {{1} \over {Z_{0}(\mu)}}p(\mu | \sigma)e^{H_{0}}
\end{align*}

with
\begin{align*}
Z_{0}(\mu) &= \sum_{\sigma} p(\mu | \sigma)e^{H_{0}}.
\end{align*}

This gives rise to the conditional expectation value:
\footnote{Normally physicists use the $\langle A\rangle$ notation
but this looks confusing when we condition on
$\mu$: $\langle A|\mu\rangle$?}

\begin{align*}
E_{0}(A | \mu) &= {{1} \over {Z_{0}(\mu)}}\sum_{\sigma} p(\mu | \sigma)e^{H_{0}(\sigma)}A(\sigma),
\end{align*}

So now we have
\begin{align*}
E_{0}(e^{V} | \mu) &= {{1} \over {Z_{0}(\mu)}}\sum_{\sigma} p(\mu | \sigma)e^{H_{0}+V}
\end{align*}

and
$$\boxed{Z = \sum_{\mu} Z_{0}(\mu)E_{0}(e^{V} | \mu)}$$

This is our magic formula, if we choose
$H_0, V, \mu$ wisely we can use it to find $Z$.

All these formula are completely general, so
now we decide what are the $H_0, V, \mu$.
We divide our lattice into blocks of three spins.
$H_0$ is interactions within a block and $V$ is
interactions between blocks.
We have one $\mu_I$ variable for each block $I$,
it is the majority vote of the three spins %$\sigma$
in that block.

We make use of the cumulant expansion to first order:
\begin{align*}
E(e^{A}) &\simeq e^{E(A)}
\end{align*}

And later we use the second order expansion:
\begin{align*}
E(e^{A}) &\simeq e^{E(A)+{{1} \over {2}}E(A^{2})-E(A)^{2}}
\end{align*}

To compute $Z_0(\mu)$:
\begin{align*}
Z_{0}(\mu) &= \sum_{\sigma} p(\mu | \sigma)e^{H_{0}}\\
 &= \sum_{\sigma} \prod_{I} p(\mu_{I} | \sigma_{I,1}, \sigma_{I,2}, \sigma_{I,3})e^{F(\sigma_{I})}\\
 &= \prod_{I} \sum_{\sigma} p(\mu_{I} | \sigma_{I,1}, \sigma_{I,2}, \sigma_{I,3})e^{F(\sigma_{I})}\\
 &= \prod_{I} f(\mu_{I})
\end{align*}

where $F(\sigma_I)$ is defined as:
\begin{align*}
F(\sigma_{I}) &= K(\sigma_{I,1}\sigma_{I,2}+\sigma_{I,1}\sigma_{I,3}+\sigma_{I,2}\sigma_{I,3})+h(\sigma_{I,1}+\sigma_{I,2}+\sigma_{I,3})
\end{align*}

and $f$ is as follows:
\begin{align*}
f(1) &= e^{3K+3h}+3e^{-K+h}
\end{align*}
\begin{align*}
f(-1) &= e^{3K-3h}+3e^{-K-h}.
\end{align*}

We can now introduce variables $A$ and $B$ such that:
\begin{align*}
f(\mu_{I}) &= e^{A+B\mu_{I}}
\end{align*}

and finally we get:
$$\boxed{Z_{0}(\mu) = \prod_{I} e^{A+B\mu_{I}}}$$


Let's write $V(\sigma)$ as
\begin{align*}
V(\sigma) &= \sum_{i,j} \chi_{i,j}\sigma_{i}\sigma_{j}
\end{align*}

Ie., $\chi$ encodes connections between blocks.

Now we compute:
\begin{align*}
E_{0}(V | \mu) &= {{1} \over {Z_{0}(\mu)}}\sum_{\sigma} p(\mu | \sigma)e^{H_{0}(\sigma)}V(\sigma)\\
 &= {{1} \over {Z_{0}(\mu)}}\sum_{\sigma} \sum_{i,j} p(\mu | \sigma)\chi_{i,j}e^{H_{0}(\sigma)}\sigma_{i}\sigma_{j}\\
 &= {{1} \over {Z_{0}(\mu)}}\sum_{\sigma} \sum_{i,j} p(\mu | \sigma)\chi_{i,j}\prod_{I} e^{F(\sigma_{I})}\sigma_{i}\sigma_{j}\\
 &= \sum_{i,j} \chi_{i,j}E(\sigma_{i}\sigma_{j} | \mu)\\
 &= \sum_{i,j} \chi_{i,j}E(\sigma_{i} | \mu)E(\sigma_{j} | \mu)
\end{align*}

This last step works because the $\sigma_i$ and $\sigma_j$ are
conditionally independant given $\mu$ whenever $\chi_{i, j}$ is
nonzero. Ie., when $i$ and $j$ refer to different blocks we get:
\begin{align*}
p_{0}(\sigma_{i} | \mu)p_{0}(\sigma_{j} | \mu) &= p_{0}(\sigma_{i}, \sigma_{j} | \mu).
\end{align*}

Now:
\begin{align*}
E_{0}(\sigma_{i} | \mu) &= {{1} \over {Z_{0}(\mu)}}\sum_{\sigma} p(\mu | \sigma)e^{H_{0}(\sigma)}\sigma_{i}
\end{align*}

Using the block index notation:
\begin{align*}
E_{0}(\sigma_{J,i} | \mu) &= {{1} \over {Z_{0}(\mu)}}\left({\prod_{I \not = J} \sum_{\sigma_{i}} p(\mu_{I} | \sigma_{I})e^{F(\sigma_{I})}}\right)\left({\sum_{\sigma_{J,1},\sigma_{J,2},\sigma_{J,3}} p(\mu_{J} | \sigma_{J})e^{F(\sigma_{J})}\sigma_{J,i}}\right)\\
 &= {{1} \over {Z_{0}(\mu)}}\left({\prod_{I \not = J} f(\mu_{I})}\right)g(\mu_{J})\\
 &= f(\mu_{J})^{-1}g(\mu_{J})
\end{align*}

with
\begin{align*}
g(1) &= e^{3K+3h}+e^{-K+h}
\end{align*}
\begin{align*}
g(-1) &= e^{3K-3h}-e^{-K-h}
\end{align*}

and finally we get:
$$\boxed{E_{0}(V | \mu) = 2K\sum_{\langle I,J\rangle} (C+D\mu_{I})(C+D\mu_{J})}$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Replica Theory}

This method is equivelant to the cavity method, and (but?)
involves formal manipulations that are not (mathematically) well defined.
See \cite{Mezard2009,Parisi2007} for more details and references.

Here we are concerned with a an ensemble of classical physics models,
distributed so that a model
has $2^N$ energy levels $E_j,\ \ 1\le j\le 2^N$. The $E_j$ are
Gaussian random variables with mean $0$ and variance $N/2.$

The partition function is then a random variable:
$$
    Z = \sum_{j=1}^{2^N} \exp(-\beta E_j).
$$

The goal here is to compute the expectation value $\Expect(\log Z).$
To do this we write
$$
    \Expect(\log Z) = \lim_{n\to 0} \frac{1}{n} \log(\Expect Z^n),
$$
calculate the integer powers of $Z$, and then ``remember'' that
$n$ was actually a real number.
\begin{align*}
    Z^n &= \sum_{i_1,...,i_n=1}^{2^N} \exp(-\beta E_{i_1} - ... -\beta E_{i_n})\\
        &= \sum_{i_1,...,i_n=1}^{2^N} \prod_{j=1}^{2^N} \exp\Bigl(-\beta E_j \sum_{a=1}^n \Ind(i_a=j)\Bigr).
\end{align*}
{\bf Question:} is it fair to call this a path-integral?

Since the $E_j$ are independent Gaussian variables,
$$
    \Expect(Z^n) = \sum_{i_1,...,i_n=1}^{2^N} \exp\Bigl(\frac{\beta^2N}{4} \sum_{a,b=1}^n \Ind(i_a=i_b)\Bigr).
$$

This expectation value can be reinterpreted as the partition
function of a new ``replicated'' system. Now the dynamical variables
are the indices $i_1,...,i_n$ and, in contrast to the original system,
this one is not being sampled from an ensemble of systems.

See also: the method of moments.

%\section{Homology/Co-Homology?}
%
%...Forward and reverse
% how is homology a path integral? is euler characteristic a path integral?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\section{Representing Compact Hausdorff spaces}


There is a vast theory of representing topological spaces as
algebraic objects, here we present one of the simplest such
instances of this
\cite{Nagata1985}.

%https://books.google.com.au/books?id=DMLSBQAAQBAJ&pg=PA132&lpg=PA132&dq=compact+space+%22lattice+of+continuous+functions%22&source=bl&ots=t5g-GKJjma&sig=xfFhgg3SPhR72Xy9fONk_I3fZxQ&hl=en&sa=X&ved=0ahUKEwjK3OL2rpnKAhUG5KYKHVhvBDUQ6AEIQDAF#v=onepage&q=compact%20space%20%22lattice%20of%20continuous%20functions%22&f=false

Let $X$ be a compact Hausdorff space.
Let $C(X)$ be the set of real valued continuous functions on $X$.
%This is a ring under pointwise addition and multiplication.
This is a lattice under the pointwise meet and join operations inherited from $\R$.

A \emph{prime ideal} of $C(X)$ is $\{x\in X:f(x)=0\}$ where $f:X\to \{0,1\}$
is any lattice homomorphism. 
The elements of the lattice $\{0, 1\}$ are also known as 
``false'' and ``true''.

{\bf Lemma.} A proper subset $J$ of $C(X)$ is a prime ideal
iff it satisfies the following:\newline
\ \ \ (1) $f\in J$ and $g\in J$ implies $f\vee g \in J,$ \newline
\ \ \ (2) $f\in J$ and $g<f$ implies $g\in J,$ \newline
\ \ \ (3) $f\notin J$ and $g\notin J$ implies $f\wedge g\notin J.$
\qed

We can associate points $x\in X$ to prime ideals $P$ as follows:
if $f\in P$ and $g(x)< f(x)$ then $g\in P.$

Given a point $x\in X$ we can build a prime ideal associated to $x$
as $\{f\in C(X) | f(x)<a\}$ where $a$ is any constant in $\R.$

{\bf Lemma.} Any prime ideal $P$ is associated with exactly one point $x\in X.$
\qed

To make the association one-to-one we
define an equivelance relation on prime ideals:
$P_1\sim P_2$ when $P_1\cap P_2$ contains a prime ideal.

{\bf Lemma.} Two prime ideals $P_1$ and $P_2$ are associated with the same
point iff $P_1\sim P_2.$
\qed

The collection of isomorphism classes of prime ideals we denote $\Delta(X).$
We also write $\Delta(x)$ for the isomorphism class associated with $x\in X.$
These will be the points of a topological space, whose topology we
define using the following closure operation.
Let $B\subset\Delta(X)$ and $q\in\Delta(X).$
Then $q\in\bar{B}$ iff 
for each $q'\in B$
we can choose a prime ideal $P(q')$ such that
$\cap_{q'\in B} P(q')$ is non-empty and contained in some other prime ideal $P\in q.$

%{\bf Lemma.} This closure operation defined on subsets of $\Delta(X)$ is the
%same as the closure operation on $X$ 
{\bf Lemma.} Let $A\subset X$ and $x\in X$. Then $x\in\bar{A}$ iff
$\Delta(x)\in\overline{\Delta(a)}$ in $\Delta(X).$
\qed

This shows that $\Delta(X)$ is homeomorphic to the original space $X.$
Furthermore, since the space $\Delta(X)$
was defined using only the lattice operations of $C(X)$ we have that
any two compact Hausdorff spaces $X$ and $Y$ are homeomorphic iff $C(X)$ and
$C(Y)$ are isomorphic as lattices.

% Notice: these prime ideals look alot like Green's functions, or
% the ``seed'' functions used to drive Dijkstra's algorithm.



%\section{Type inference?}
% Haskell can infer the result type of a function from its argument (forward), or the argument type from the result (backward)

% Gaussian elimination? Euclids algorithm? Buchberger's algorithm?

% Membrane computing ?

% AdS-CFT correspondence

% quantum field theory, Feynman Vs. Schwinger 

% Does replica theory look at all like ensemble methods in machine learning (eg. boosting)... ?

% Nerual networks look like renormalization group... so does RG do any ``backpropagation'' ?
% \cite{Mehta2014}

% Path integrals are famously ill-defined... eg. Feynman, Witten, Replica theory...
%    -- is it possible that this is a manifestation of (Godelian) incompleteness ?


% Game theory: ``path integrals'' over (rooted) trees (maybe not quite trees..)
%    -- monte-carlo tree search
%    -- alpha-beta pruning

% Timeseries analysis: gaussian processes (???), Black-Scholes option pricing formula (???)

% Is Action (kinetic-potential energy) an Euler characteristic?
%    -- kinetic energy is "1-dimensional"
%    -- potential energy is "0-dimensional"

% Species give a power series: \sum f(n) x^n
% Wave propagation looks like this \sum f(w) e^{iw \theta}
%   -- is species a kind of wavefront?
%

% Historical: 
% https://en.wikipedia.org/wiki/Project_Cybersyn
% http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/

% Options pricing is a kind of path integral:
%    https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_equation
%    https://en.wikipedia.org/wiki/Diffusion_equation
%    https://en.wikipedia.org/wiki/Feynman%E2%80%93Kac_formula
%    https://en.wikipedia.org/wiki/It%C3%B4%27s_lemma
%    https://en.wikipedia.org/wiki/Martingale_(probability_theory)

% Yang-Baxter: $$\sigma_1 ( \sigma_2 \sigma_1 ) = \sigma_2 ( \sigma_1 \sigma_2 )$$
% ?




\bibliography{refs}{}
\bibliographystyle{abbrv}


\end{document}

